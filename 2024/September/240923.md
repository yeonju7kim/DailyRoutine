# SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data

읽은 이유: Text를 Phomene으로 바꾸는 Tokenizer 어떻게 학습하는거지?

요약
- Speech와 Text의 alignment를 맞춤
- Phoneme-unit Tokenizer와 Hidden-unit Tokenizer
- Pretraining task로
  - Unit-based masked language modeling(UMLM)
  - Unit-based connectionist temporal classification(UCTC)
  - Random swapping mechanism

어떻게 학습되었는지
- Phoneme-unit Tokenizer와 Hidden-unit Tokenizer
- Speech Transformer(HuBERT)
  -  
