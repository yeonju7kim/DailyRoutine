# Auto-Encoding Morph-Tokens for Multimodal LLM

읽은 이유: 수업

기존에는 mllm 전과 후의 token이 같았음
하지만, generation을 할 떄는 모든 semantic을 preserve해야하고, comprehension을 할 때는 abstract해야 되는데, 그러기 위해서 morph token은 mllm 전 후로 다를 수 있음

![image](https://github.com/user-attachments/assets/4b17f9fd-7e58-41a3-8db3-270dc92ff700)

architecture
- encoder: CLIP-ViT, Q-former, Quantizer
- morph-token-based MLLM: Vicuna
- decoder: transformer + VQGAN
- 
Training
stage1. image-text dataset으로 LLM과 morph token 학습
stage2. morph token을 auto-encode (generation 학습하려는 듯)
stage3. instruct tuning

![image](https://github.com/user-attachments/assets/0e376874-a532-47c3-9d67-038b10c7817d)

이미지 editing 벤치마크 첨봄
