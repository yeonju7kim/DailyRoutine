# VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning
https://arxiv.org/abs/2211.11275
읽은 이유: text encoder를 어떻게 학습시켜야 되지... 

- masked prediction task of unified tokens
  
![image](https://github.com/user-attachments/assets/ed17d5e1-4b1a-4bb8-91d5-98a64f553dfd)
학습 데이터 만들 때, phoneme2unit model 필요함. (d)처럼 Text encoder, duration module, unit decoder 로 구성되어 있는 것을 ㅈ거은 양의 speech transcription data로 학습함.

