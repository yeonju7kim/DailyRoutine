# Learning audio-visual speech representation by masked multimodal cluster prediction

읽은 이유: 알고 쓸려고 + multilingual ASR 등에서 pretrained model 학습에 대한 아이디어를 얻으려고?

- Motivation: Cross modal correlation은 downstream performance에 도움이 된다.
- AVHuBERT로 lingustic and phonetic information + long-range temporal relationships 등의 정보를 얻을 수 있다.
- Method
  - Feature clustering + masked prediction
  - masked prediction을 함으로써 good local acoustic representation, long-range temporal dependency를 배울 수 있다.
  - speech representation에 audio feature가 더 적합하다고 생각해서 HuBERT의 knowledge distillation처럼 학습됨
- Pretraining
  - Audio-Visual input(fused) + modality dropout(sequence level)
  - Audio-Visual clustering
  - Masking by substitution(fake frame을 찾는 것. 아직 잘 이해 못함)
- Finetuning
  - connectionist temporal classification(CTC)
  - attention-based sequence-to-sequence corss-entropy loss(S2S)
- backbone model로는 ROI, log filterbank energy function
- encoder로는 resnet(image), linear projection layer(audio), transformer로 fuse
- 실험에서 multilingual로 할 때 더 좋았음.
